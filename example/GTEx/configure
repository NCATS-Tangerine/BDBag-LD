#!/bin/bash

set -e
set -x

# set the directory of the raw variant data
VAR=./GTEx_data

DEST=.

if [ ! -z $1 ]; then
    VAR=$1
fi

# the list of tissues to process
tissues=(
"Adipose_Subcutaneous, 0002190")

alltissues=(
"Adipose_Subcutaneous, 0002190"
"Adipose_Visceral_Omentum, 0003688"
"Adrenal_Gland, 0018303"
"Artery_Aorta, 0004178"
"Artery_Coronary ,0002111"
"Artery_Tibial, 0007610"
"Brain_Amygdala, 0001876"
"Brain_Anterior_cingulate_cortex_BA24, 0009835"
"Brain_Caudate_basal_ganglia, 0002420"
"Brain_Cerebellar_Hemisphere, 0002245"
"Brain_Cerebellum, 0002037"
"Brain_Cortex, 0000956"
"Brain_Frontal_Cortex_BA9, 0001870"
"Brain_Hippocampus, 0002310"
"Brain_Hypothalamus, 0001898"
"Brain_Nucleus_accumbens_basal_ganglia, 0001882"
"Brain_Putamen_basal_ganglia, 0001874"
"Brain_Spinal_cord_cervical_c-1, 0002726"
"Brain_Substantia_nigra, 0002038"
"Breast_Mammary_Tissue, 0001911"
"Cells_EBV-transformed_lymphocytes, 0001744"
"Cells_Transformed_fibroblasts, 0015764"
"Colon_Sigmoid, 0001159"
"Colon_Transverse, 0001157"
"Esophagus_Gastroesophageal_Junction, 0007650"
"Esophagus_Mucosa, 0002469"
"Esophagus_Muscularis, 0004648"
"Heart_Atrial_Appendage, 0006618"
"Heart_Left_Ventricle, 0002084"
"Liver, 0002107"
"Lung, 0002048"
"Minor_Salivary_Gland, 0001830"
"Muscle_Skeletal, 0001134"
"Nerve_Tibial, 0001323"
"Ovary, 0000992"
"Pancreas, 0001264"
"Pituitary, 0000007"
"Prostate, 0002367"
"Skin_Not_Sun_Exposed_Suprapubic, 0036149"
"Skin_Sun_Exposed_Lower_leg, 0004264"
"Small_Intestine_Terminal_Ileum, 0002116"
"Spleen, 0002106"
"Stomach, 0000945"
"Testis, 0000473"
"Thyroid, 0002046"
"Uterus, 0000995"
"Vagina, 0000996"
"Whole_Blood, 0000178")

# gets the raw data from the GTEx website and unpacks it.
get_archives ()
{
    # remove the raw data directory if it exists
    rm -rf $VAR

    # make the target data directory
    mkdir -p $VAR

    # save where we are now
    cur_dir=$PWD

    # go to the raw data directory
    cd $VAR

     # set the URL to the data. only going after the single tiussue data for now
    download_prefix="https://storage.googleapis.com/gtex_analysis_v7/single_tissue_eqtl_data"

    # list of raw archive files
    # note although we are setting up for processing a list only one file is needed right now.
    # there are numerous other files that may also be desired on the site in the future.
    archive_files="GTEx_Analysis_v7_eQTL"

    # for each raw data archive file
    for f in $archive_files; do
	# checkpoint for the user
        echo $f

	# make the call to get the data
#        wget --timestamping $download_prefix/$f.tar.gz
        wget --quiet --timestamping $download_prefix/$f.tar.gz
    done

    # now unpack them
    for f in $archive_files; do
	# does the file exist
        if [ -f $f.tar.gz ]; then
	    # extract the files. the data is actually in a subdir in the archive
	    tar -zxvf $f.tar.gz $f -C ./

	    # move the tissue files back to this working directory
           mv ./$f/*.gz ./

	    # remove the temp directory
	    rm -rf $f
        else
            echo "no such file $f.tar.gz"
            exit 1
        fi
    done

    # Process each tissue archive file
    for tissue in "${tissues[@]}"; do
        # split the tissue data to get the name
        IFS=', ' read -ra tissue_data <<< $tissue

        # something fo rthe user
        echo Including ${tissue_data[0]} in $VAR.

        # The tissues are a compressed archive. Uncompress it
        if [ -f ${tissue_data[0]}.v7.egenes.txt.gz ]; then
            gunzip -c ${tissue_data[0]}.v7.egenes.txt.gz > ${tissue_data[0]}.v7.egenes.csv
        else
            echo "no such file ${tissue_data[0]}.v7.egenes.txt.gz"
            exit 1
        fi

        # The tissues are a compressed archive. Uncompress it
        if [ -f ${tissue_data[0]}.v7.signif_variant_gene_pairs.txt.gz ]; then
            gunzip -c ${tissue_data[0]}.v7.signif_variant_gene_pairs.txt.gz > ${tissue_data[0]}.v7.signif_variant_gene_pairs.csv
        else
            echo "no such file $tissue.v7.signif_variant_gene_pairs.txt.gz"
            exit 1
        fi
    done

    # remove all intermediate files
    rm ./*.gz

    # go back to the root directory
    cd $cur_dir
}

# here we create CSV files from the txt data starting with adding a header record then 
# appending all data from each tissue into a merged CSV file.
# Finally create a bag by copying required data files into the bag.
stage_data () {
    # get the tissues list
    tissues="$1"

    # save the current directory
    cur_dir=$PWD

    # make sure we are in the root GTEx directory
    cd $DEST

    # remove the bag directory and start from scratch
    rm -rf bag

    # make the bag directory
    mkdir bag

    # here we start the process of merging all the csv files into the two types

    # add the column headers to the target egenes file
    echo 'tissue_name,tissue_uberon,gene_id,gene_name,gene_chr,gene_start,gene_end,strand,num_var,beta_shape1,beta_shape2,true_df,pval_true_df,variant_id,tss_distance,chr,pos,ref,alt,num_alt_per_site,rs_id_dbSNP147_GRCh37p13,minor_allele_samples,minor_allele_count,maf,ref_factor,pval_nominal,slope,slope_se,pval_perm,pval_beta,qval,pval_nominal_threshold,log2_aFC,log2_aFC_lower,log2_aFC_upper' > bag/egenes.csv

    # add the column headers to the target signif file
    echo 'tissue_name,tissue_uberon,variant_id,gene_id,tss_distance,ma_samples,ma_count,maf,pval_nominal,slope,slope_se,pval_nominal_threshold,min_pval_nominal,pval_beta' >  bag/signif_variant_gene_pairs.csv

    # for each tissue file
    for tissue in "${tissues[@]}"; do
        # split the tissue data into  the name removing the underscores
        IFS=', ' read -ra tissue_data <<< "$tissue"

	# create the column data for the tissue name and uberon
	tissueName="${tissue_data[0]//_/ },"

        # create the column data for the uberon value
	tissueUberon="${tissue_data[1]},"

        # init a line counter for the egenes records
        i=1

        # for each line in the file except for the first one
        while read line
        do
            # skip the first line
            test $i -eq 1 && ((i=i+1)) && continue

            # write out the line prepending with the tissue type and replacing tabs with commas
            echo $tissueName$tissueUberon${line//	/,} >> bag/egenes.csv

	# load up the next file to process
        done < $VAR/${tissue_data[0]}.v7.egenes.csv

        # init a line counter for the signif records
        i=1

        # for each line in the file except for the first one
        while read line
        do
            # skip the first line
            test $i -eq 1 && ((i=i+1)) && continue

            # write out the line prepending with the tissue type and replacing tabs with commas
            echo $tissueName$tissueUberon${line//	/,} >> bag/signif_variant_gene_pairs.csv

	# load up the next file to process
        done < $VAR/${tissue_data[0]}.v7.signif_variant_gene_pairs.csv
    done

    # move to the target directory
    cd $cur_dir
}

###################
# create the metadata annotations
###################
stage_metadata () {
    # save the current directory
    cur_dir=$PWD

    # make sure we are in the root GTEx directory
    cd $DEST

    # create the bag directories for annotations and provenance
    mkdir -p bag/metadata/annotations
    mkdir -p bag/metadata/provenance

    # get a copy of the json data descriptors into the metdata annotation dir
    cp $cur_dir/proj_files/schema/*.jsonld bag/metadata/annotations/

    # save the data provenance directory
    cp -r $cur_dir/proj_files/provenance bag/metadata/

    # save the data manifest
    cp -r $cur_dir/proj_files/manifest.json bag/metadata/

    # save the run options
    cp -r $cur_dir/proj_files/options.json ./
}

build_it () {
    # get the raw data from the archive
    get_archives

    # Stage data into a bag directory.
    stage_data "$tissues"

    # Generate bag structure and metadata.
#    bdbag $DEST/bag

    # Stage metadata to the bag.
    # Adding before structuring the bag with bdbag results in the metadata directory being included in the
    # data payload directory which is not conformant to the spec. So we add it here.
#    stage_metadata "$tissues"

    # Archive the bag and associated metadata.
#    bdbag $DEST/bag/ --update --validate fast --validate-profile --archive tgz

    # List bag contents for sanity check.
#    tar tf bag.tgz
}

build_it

exit 0
