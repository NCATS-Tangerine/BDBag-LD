#!/bin/bash

set -e
set -x

# set the directory of the raw variant data
VAR=./GTEx_data

DEST=.

if [ ! -z $1 ]; then
    VAR=$1
fi

# the list of tissues to process
onetissues="
Adipose_Subcutaneous"

tissues="
Adipose_Subcutaneous
Adipose_Visceral_Omentum
Adrenal_Gland
Artery_Aorta
Artery_Coronary
Artery_Tibial
Brain_Amygdala
Brain_Anterior_cingulate_cortex_BA24
Brain_Caudate_basal_ganglia
Brain_Cerebellar_Hemisphere
Brain_Cerebellum
Brain_Cortex
Brain_Frontal_Cortex_BA9
Brain_Hippocampus
Brain_Hypothalamus
Brain_Nucleus_accumbens_basal_ganglia
Brain_Putamen_basal_ganglia
Brain_Spinal_cord_cervical_c-1
Brain_Substantia_nigra
Breast_Mammary_Tissue
Cells_EBV-transformed_lymphocytes
Cells_Transformed_fibroblasts
Colon_Sigmoid
Colon_Transverse
Esophagus_Gastroesophageal_Junction
Esophagus_Mucosa
Esophagus_Muscularis
Heart_Atrial_Appendage
Heart_Left_Ventricle
Liver
Lung
Minor_Salivary_Gland
Muscle_Skeletal
Nerve_Tibial
Ovary
Pancreas
Pituitary
Prostate
Skin_Not_Sun_Exposed_Suprapubic
Skin_Sun_Exposed_Lower_leg
Small_Intestine_Terminal_Ileum
Spleen
Stomach
Testis
Thyroid
Uterus
Vagina
Whole_Blood"

# gets the raw data from the GTEx website and unpacks it.
get_archives ()
{
    # remove the raw data directory if it exists
    rm -rf $VAR

    # make the target data directory
    mkdir -p $VAR

    # save where we are now
    cur_dir=$PWD

    # go to the raw data directory
    cd $VAR

     # set the URL to the data. only going after the single tiussue data for now
    download_prefix="https://storage.googleapis.com/gtex_analysis_v7/single_tissue_eqtl_data"

    # list of raw archive files
    # note although we are setting up for processing a list only one file is needed right now.
    # there are numerous other files that may also be desired on the site in the future.
    archive_files="
	GTEx_Analysis_v7_eQTL
    "

    # for each raw data archive file
    for f in $archive_files; do
	# checkpoint for the user
#        echo $f

	# make the call to get the data
#        wget --timestamping $download_prefix/$f.tar.gz
        wget --quiet --timestamping $download_prefix/$f.tar.gz
    done

    # now unpack them
    for f in $archive_files; do
	# does the file exist
        if [ -f $f.tar.gz ]; then
	    # extract the files. the data is actually in a subdir in the archive
	    tar -zxvf $f.tar.gz $f -C ./

	    # move the tissue files back to this working directory
            mv ./$f/*.gz ./

	    # remove the temp directory
	    rm -rf $f
        else
            echo "no such file $f.tar.gz"
            exit 1
        fi
    done

    # Process each tissue archive file
    for tissue in $tissues; do
        echo Including $tissue in $VAR.

        # The tissues are a compressed archive. Uncompress it
        if [ -f $tissue.v7.egenes.txt.gz ]; then
            gunzip -c $tissue.v7.egenes.txt.gz > $tissue.v7.egenes.csv
        else
            echo "no such file $tissue.v7.egenes.txt.gz"
            exit 1
        fi

        # The tissues are a compressed archive. Uncompress it
        if [ -f $tissue.v7.signif_variant_gene_pairs.txt.gz ]; then
            gunzip -c $tissue.v7.signif_variant_gene_pairs.txt.gz > $tissue.v7.signif_variant_gene_pairs.csv
        else
            echo "no such file $tissue.v7.signif_variant_gene_pairs.txt.gz"
            exit 1
        fi
    done

    # remove all intermediate files
    rm ./*.gz

    # go back to the root directory
    cd $cur_dir
}

# here we create CSV files from the txt data starting with adding a header record then 
# appending all data from each tissue into a merged CSV file.
# Finally create a bag by copying required data files into the bag.
stage_data () {
    # get the tissues list
    tissues="$1"

    # save the current directory
    cur_dir=$PWD

    # make sure we are in the root GTEx directory
    cd $DEST

    # remove the bag directory and start from scratch
    rm -rf bag

    # make the bag directory
    mkdir bag

    # here we start the process of merging all the csv files into the two types

    # add the column headers to the target egenes file
    echo 'tissue_name,gene_id,gene_name,gene_chr,gene_start,gene_end,strand,num_var,beta_shape1,beta_shape2,true_df,pval_true_df,variant_id,tss_distance,chr,pos,ref,alt,num_alt_per_site,rs_id_dbSNP147_GRCh37p13,minor_allele_samples,minor_allele_count,maf,ref_factor,pval_nominal,slope,slope_se,pval_perm,pval_beta,qval,pval_nominal_threshold,log2_aFC,log2_aFC_lower,log2_aFC_upper' > bag/egenes.csv

    # add the column headers to the target signif file
    echo 'tissue_name,variant_id,gene_id,tss_distance,ma_samples,ma_count,maf,pval_nominal,slope,slope_se,pval_nominal_threshold,min_pval_nominal,pval_beta' >  bag/signif_variant_gene_pairs.csv

    # for each tissue file 
    for tissue in $tissues; do
	# create the column data for the tissue name
	tissueName="${tissue//_/ },"

        # init a line counter for the egenes records
        i=1

        # for each line in the file except for the first one
        while read line
        do
            # skip the first line
            test $i -eq 1 && ((i=i+1)) && continue

            # write out the line prepending with the tissue type and replacing tabs with commas
            echo $tissueName${line//	/,} >> bag/egenes.csv

	# load up the next file to process
        done < $VAR/$tissue.v7.egenes.csv

        # init a line counter for the signif records
        i=1

        # for each line in the file except for the first one
        while read line
        do
            # skip the first line
            test $i -eq 1 && ((i=i+1)) && continue

            # write out the line prepending with the tissue type and replacing tabs with commas
            echo $tissueName${line//	/,} >> bag/signif_variant_gene_pairs.csv

	# load up the next file to process
        done < $VAR/$tissue.v7.signif_variant_gene_pairs.csv
    done

    # move to the target directory
    cd $cur_dir
}

###################
# create the metadata annotations
###################
stage_metadata () {
    # save the current directory
    cur_dir=$PWD

    # make sure we are in the root GTEx directory
    cd $DEST

    # create the bag directories for annotations and provenance
    mkdir -p bag/metadata/annotations
    mkdir -p bag/metadata/provenance

    # get a copy of the json data descriptors into the metdata annotation dir
    cp $cur_dir/proj_files/schema/*.jsonld bag/metadata/annotations/

    # save the data provenance directory
    cp -r $cur_dir/proj_files/provenance bag/metadata/

    # save the data manifest
    cp -r $cur_dir/proj_files/manifest.json bag/metadata/

    # save the run options
    cp -r $cur_dir/proj_files/options.json ./
}

build_it () {
    # get the raw data from the archive
#    get_archives

    # Stage data into a bag directory.
#    stage_data "$tissues"

    # Generate bag structure and metadata.
#    bdbag $DEST/bag

    # Stage metadata to the bag.
    # Adding before structuring the bag with bdbag results in the metadata directory being included in the
    # data payload directory which is not conformant to the spec. So we add it here.
    stage_metadata "$tissues"

    # Archive the bag and associated metadata.
#    bdbag $DEST/bag/ --update --validate fast --validate-profile --archive tgz

    # List bag contents for sanity check.
#    tar tf bag.tgz
}

build_it

exit 0
